---
title: "Denoising and decontaminating Scorpion SMR MS"
output: html_notebook
date: 3-30-2020
Author: Zack Gold
Code Assistance: Ryan Kelly & Ramon Gallego
---
# Load the dataset and metadata

```{r load libraries, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)

library (tidyverse)
library (vegan)
library (proxy)
library(reshape2)
library(microDecon)
```

```{r How.many}
how.many <- function(ASVtable, round){
  ASVtable %>% ungroup() %>% 
    dplyr::summarise(nsamples = n_distinct(sample),
              nHashes = n_distinct(Hash),
              nReads = sum(nReads), 
              Stage = paste0("Step_", round)) %>% 
    gather(starts_with("n"), value = "number", key = "Stat")
}
```

We will load the ASV table and the metadata file. They are in the same folder so we use `list.files` to access them and a neat combination of `bind.rows` and `map(read_csv)`

```{r load datasets - we will be doing that for all runs}
setwd("/Users/zackgold/Documents/UCLA_phd/Projects/California/General_Channel_Islands/MPA_test_data/input/2020320_analysis/decontam")

#Paths to Files
input_biom_path <- "/Users/zackgold/Documents/UCLA_phd/Projects/California/General_Channel_Islands/MPA_test_data/input/2020320_analysis/combined_data/c19_fishcard_taxonomy_tables/Summary_by_percent_confidence/60/c19_fishcard_ASV_raw_taxonomy_60_edited.txt"

input_meta_path <- "/Users/zackgold/Documents/UCLA_phd/Projects/California/General_Channel_Islands/MPA_test_data/input/jan_2020_analysis/decontam/mpa_metadata_02042020.txt"

input_hash_path <- "/Users/zackgold/Documents/UCLA_phd/Projects/California/General_Channel_Islands/MPA_test_data/input/2020320_analysis/decontam/mpa_test_hashes_033020.txt"
```

```{r}
mpa_taylor <- read.table(input_biom_path, header = 1, sep = "\t", stringsAsFactors = F)
mpa_taylor$Miseq_run <- "miu"
miu_names <- colnames(mpa_taylor)

#Merge All Tables
ASV.table <- mpa_taylor

#Format for Long Data
ASV.table$seq_number <- factor(ASV.table$seq_number)
ASV.table$Miseq_run <- factor(ASV.table$Miseq_run)

columns <- colnames(ASV.table)
remove <- c("seq_number","sum.taxonomy","Miseq_run")

gathercols <-  columns[! columns %in% remove] 

# Convert to Long Data
ASV.table <- gather(ASV.table, sample, reads, gathercols, factor_key=TRUE)
ASV.table$reads <- as.numeric(ASV.table$reads)

metadata <- read.table(input_meta_path, header = 1, sep = "\t", stringsAsFactors = F)

Hash.key <- read.table(input_hash_path, header = 1, sep = "\t", stringsAsFactors = F)
```

The output of this process are a clean ASV table and a clean metadata file.

## Cleaning Process 1: Estimation of *Tag-jumping* or sample *cross-talk*

Before we modify our datasets on any way, we can calculate how many sequences that were only supposed to be in the positives control appeared in the environmental samples, and how many did the opposite. First we divide the dataset into positive control and environmental samples. Also create an ordered list of the Hashes present in the positive controls, for ease of plotting

```{r split into two}

#Remove Singletons (since we can not use them ever)
#ASV.table %>%
#  dplyr::group_by(seq_number) %>%
#  mutate (TotalReadsperSample = sum(reads)) %>% 
#  filter(., TotalReadsperSample > 1) %>% 
#  dplyr::select(-TotalReadsperSample) -> ASV.table

#Create list of control samples
metadata %>% 
  filter(Sample_Control=="control") %>% 
  dplyr::select(Samples) -> controls
controls <- controls$Samples

metadata %>% 
  filter(Control_type=="Pos") %>% 
  dplyr::select(Samples) -> pos_controls
pos_controls <- pos_controls$Samples

metadata %>% 
  filter(Control_type=="Neg") %>% 
  dplyr::select(Samples) -> neg_controls
neg_controls <- neg_controls$Samples

#Create list of Taylor and MPA Samples
metadata %>% 
  filter(Site=="Taylor") %>% 
  dplyr::select(Samples) -> Taylor_samples
 Taylor_samples <-  Taylor_samples$Samples

#New column that labels each ASV as from Positive (control) or Sample
ASV.table %>% 
  mutate(source = case_when(sample %in% pos_controls~"Positives",
                            sample %in% neg_controls~"Blanks",
                             TRUE ~"Samples")) -> ASV.table
  
#Convert to tibble
ASV.table <- as_tibble(ASV.table)

#Remove empty sequences
ASV.table %>% 
  filter(reads != 0)  -> ASV.table

#Rename Columns and remove seq_number
ASV.table %>%
  mutate(Hash = as.character(seq_number),
         sample = as.character(sample),
         nReads = reads) %>% 
  dplyr::select(-seq_number)  -> ASV.table

ASV.table %>% 
  filter (source != "Samples") %>%
  dplyr::group_by(Hash) %>% 
  dplyr::summarise(tot = sum(reads)) %>% 
  arrange(desc(tot)) %>% 
  pull(Hash) -> all.seqs.in.ctrls

Hash.key %>% 
  filter(seq_number %in% all.seqs.in.ctrls) %>% as.tibble() -> contam.species

```

```{r}
ASV.table %>% 
  group_by(sample) %>%
  filter(., Miseq_run=="miu") %>% 
  mutate (TotalReadsperSample = sum(nReads)) %>%
  arrange(desc(TotalReadsperSample)) %>%
  ggplot(., aes(x=sample, y=TotalReadsperSample, color=source)) + geom_point() +ggtitle("Read Count Across Samples") + theme(axis.text.x = element_text(angle = 90))
```
Now let's create a jumping vector. What proportion of the reads found in the positives control come from elsewhere, and what proportion of the reads in the samples come from the positives control.
### Step 1: Nest the dataset and split it in positives and samples

To streamline the process and make it easier to execute it similarly but independently on each Miseq run, we nest the dataset by run. 
So Step1 is create a nested table so we can run this analysis on each run independently. 


```{r nesting the dataset}
ASV.table %>% 
  dplyr::group_by(Miseq_run, source) %>% 
  nest() %>% 
  pivot_wider(names_from=source, values_from=data) -> ASV.nested
```

That wasn't too complicated. Let's start a summary function that keeps track of our cleaning process


```{r summary.file}
ASV.nested %>% 
  ungroup() %>% 
  dplyr::transmute(.,Miseq_run,Summary = purrr::map(Samples, ~ how.many(ASVtable = ., round = 0)))  -> ASV.summary

ASV.summary$Summary
#Elas had some samples in which all reads were removed
```




```{r}
ASV.table %>% 
  filter(!sample %in% Taylor_samples) %>% 
  dplyr::group_by(Miseq_run, source) %>% 
  nest() %>% 
  pivot_wider(names_from=source, values_from=data) -> ASV.nested.mpa_1

ASV.nested.mpa_1 %>%  
  ungroup() %>%  
  dplyr::transmute(.,Miseq_run,Summary = purrr::map(Samples, ~ how.many(ASVtable = ., round = 0)))  -> ASV.summary.mpa_1

ASV.summary.mpa_1$Summary

```


### Step 2: Model the composition of the positive controls of each run 


We create a vector of the composition of each positive control and substract it from the environmental samples from their runs

```{r jumping vector}

ASV.nested %>% 
  mutate (contam.tibble = purrr::map(Positives, 
                              function(.x){
                                .x %>%
                                  ungroup() %>% 
                                  group_by(sample) %>%
                                  mutate (TotalReadsperSample = sum(nReads)) %>%
                                  mutate (proportion = nReads/TotalReadsperSample) %>%
                                  group_by(Hash) %>%
                                  dplyr::summarise (vector_contamination = max(proportion))
                                }) ) -> ASV.nested

ASV.nested$contam.tibble %>% as.data.frame() %>% 
  ggplot(aes(x= vector_contamination))+
  geom_histogram()# Check how it looks like
```


### Step 3: Substract the composition of the positive controls from the environment samples

The idea behind this procedure is that we know, for each run, how many reads from each Hash appeared in teh positive controls. These come from 2 processes: sequences we know should appear in the positive controls, and sequences that have *jumped* from the environment to the positive controls. With this procedure, we substract from every environmental sample the proportion of reads that jumped from elsewhere.

```{r cleaning step 1}
ASV.nested %>% 
  ungroup() %>% 
  mutate(cleaned.tibble = map2(Samples, contam.tibble, function(.x,.y){ 
    .x %>%
      dplyr::group_by (sample) %>%
      mutate (TotalReadsperSample = sum (nReads)) %>%
      left_join(.y, by = "Hash") %>%
      mutate (Updated_nReads = ifelse (!is.na(vector_contamination),  nReads - (ceiling(vector_contamination*TotalReadsperSample)), nReads)) %>%
      filter (Updated_nReads > 0) %>%
      ungroup() %>% 
      dplyr::select (sample, Hash, nReads = Updated_nReads)
  })) -> ASV.nested


ASV.nested$cleaned.tibble %>% as.data.frame() %>% 
  arrange(desc(nReads)) %>% head(n=100) #Check how they look

```
Add this step to the summary table we were creating

```{r summary.file.2}
ASV.nested %>% 
  transmute(Miseq_run, Summary.1 = purrr::map(cleaned.tibble, ~ how.many(ASVtable = .,round = "1.Jump"))) %>% 
  left_join(ASV.summary) %>% #use left join when there are many miseq runs to join
  bind_cols(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 

ASV.summary$Summary
```

## Cleaning Process 2: **Discarding PCR replicates with low number of reads**

We will fit the number of reads assigned to each sample to a normal distribution and discard those samples with a probability of 95% of not fitting in that distribution. The output would be a dataset with less samples and potentially less number of unique Hashes.

```{r fitting nReads per sample}

ASV.nested$cleaned.tibble %>% as.data.frame() %>% 
  group_by(sample) %>%
  dplyr::summarise(tot = sum(nReads)) %>% 
  arrange(desc(tot))-> all.reps

# Visualize

all.reps %>%  
  pull(tot) -> reads.per.sample

names(reads.per.sample) <- all.reps %>% pull(sample)  

normparams.reads <- MASS::fitdistr(reads.per.sample, "normal")$estimate

all.reps %>%  
  mutate(prob = pnorm(tot, normparams.reads[1], normparams.reads[2])) -> all.reps

#  probs <- pnorm(all_pairwise_distances, normparams[1], normparams[2])

outliers <- all.reps %>% 
  filter(prob < 0.05  & tot < normparams.reads[1]) # changed to 0.05 to save the two samples

ASV.nested %>% 
  mutate(Step.1.low.reads = purrr::map (cleaned.tibble, ~ filter(.,!sample %in% outliers$sample) %>% ungroup)) -> ASV.nested

ASV.nested %>% 
  transmute(Miseq_run, Summary.1 = purrr::map(Step.1.low.reads, ~ how.many(ASVtable = .,round = "2.Low.nReads"))) %>% 
  left_join(ASV.summary) %>% 
  bind_cols(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 

ASV.summary$Summary
```




## Cleaning Process 3: **Full clearance from Positive control influence**
We will follow microDecon here
### Remove the positive control hashes from the composition of the ASVs

```{r cleaning.Step2}
ASV.nested$Blanks[[1]] %>% 
  select(sample,Hash, nReads) -> blankers
rbind(blankers, ASV.nested$Step.1.low.reads[[1]]) -> step.1.1

step.1.1 %>% 
pivot_wider(names_from=sample, values_from=nReads, values_fill = list(nReads =0)) -> step.1.1_wide

step.1.1_wide %>%
  mutate(seq_number=Hash) %>% 
  left_join(Hash.key) %>%
  select(-seq_number) -> step.1.1_wide

as.data.frame(step.1.1_wide) -> step.1.1_wide

step.1.1_decon <- decon(data=step.1.1_wide, numb.blanks = 5,numb.ind=as.vector(c(3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3)), taxa = T)

step.1.1_decon$decon.table

columns <- colnames(step.1.1_decon$decon.table)
remove <- c("Hash","sum.taxonomy")

gathercols <-  columns[! columns %in% remove] 

# Convert to Long Data
step.1.1_decon_clean <- gather(step.1.1_decon$decon.table, sample, nReads, gathercols, factor_key=TRUE)

step.1.1_decon_clean %>% as.tibble() %>% 
  select(-sum.taxonomy) %>% 
  filter(., sample != "Mean.blank") %>% 
  filter(., nReads >0) -> step.1.1_decon_clean_tibble
class(step.1.1_decon_clean_tibble)

ASV.nested %>% 
  mutate(Step2.tibble = list(step.1.1_decon_clean_tibble)) -> ASV.nested
```

```{r Save Files Before Occupancy}
step.1.1_decon$OTUs.removed
saveRDS(step.1.1_decon, file="decon_table")

saveRDS(ASV.nested, file = "Cleaning.before.Occ.model")

ASV.nested %>% 
  transmute(Miseq_run, Summary.1 = purrr::map(Step2.tibble, ~ how.many(ASVtable = .,round = "3.Positives"))) %>%
  left_join(ASV.summary) %>% 
  bind_cols(ASV.summary) %>% 
  mutate(Summary = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary

ASV.summary$Summary
saveRDS(ASV.summary, file = "ASV.summary.rds")

```

```{r}
ASV.nested <- readRDS(file ="Cleaning.before.Occ.model")
ASV.summary <- readRDS(file ="ASV.summary.rds")
```

#Split Taylor and Zack's data
```{r}

ASV.nested$Step2.tibble %>% as.data.frame() %>% filter(., nReads>0) %>% 
  ungroup() %>% 
  filter(sample %in% Taylor_samples) -> taylor_step2.tibble

ASV.nested$Step2.tibble %>% as.data.frame() %>% filter(., nReads>0) %>% 
  ungroup() %>% 
  filter(!sample %in% Taylor_samples) -> mpa_step2.tibble

```

```{r output file before occupancy}

ASV.nested$Step2.tibble %>% 
  as.data.frame() %>% 
  mutate(.,seq_number=Hash) %>%  
  left_join(Hash.key) %>% 
  dplyr::group_by(sum.taxonomy, sample) %>% 
  dplyr::summarise(., total_reads = sum(nReads)) %>% #sum by taxonomy per site
  spread(., key = "sample", value = "total_reads", fill = 0) -> pre_occ_sum.taxonomy

saveRDS(pre_occ_sum.taxonomy, file = "preoccupancy.sum.taxonomy.rds")
write_csv(pre_occ_sum.taxonomy ,"preoccupancy.sum.taxonomy.csv")

mpa_step2.tibble %>% 
  as.data.frame() %>% 
  mutate(.,seq_number=Hash) %>%  
  left_join(Hash.key) %>% 
  dplyr::group_by(seq_number, sample) %>% 
  dplyr::summarise(., total_reads = sum(nReads)) %>% #sum by taxonomy per site
  spread(., key = "sample", value = "total_reads", fill = 0) -> mpa_pre_occ_ASV

saveRDS(mpa_pre_occ_ASV, file = "mpa_preoccupancy.ASV.decontam.rds")
write_csv(mpa_pre_occ_ASV ,"mpa_preoccupancy.ASV.decontam.csv")


mpa_step2.tibble %>% 
  mutate(.,seq_number=Hash) %>%  
  left_join(Hash.key) %>% 
  dplyr::group_by(sum.taxonomy, sample) %>% 
  dplyr::summarise(., total_reads = sum(nReads)) %>% #sum by taxonomy per site
  spread(., key = "sample", value = "total_reads", fill = 0) -> sum.taxonomy.mpa

saveRDS(sum.taxonomy.mpa, file = "mpa_pre_occ.sum.taxonomy.rds")
write_csv(sum.taxonomy.mpa ,"mpa_pre_occ.sum.taxonomy.csv")

```

## Cleaning Process 4: **Occupancy modelling**

```{r importing Occ results}

mpa_occupancy_results_all <- readRDS(file="mpa_occupancy_results_all.RDS")
mpa_occupancy_results_75 <- readRDS(file="mpa_occupancy_results_75.RDS")

ASV.nested <- readRDS(file ="Cleaning.before.Occ.model")
ASV.summary <- readRDS(file ="ASV.summary.rds")

```

```{r Visualize SOM}
#View Histogram
mpa_occupancy_results_all %>% 
  pull(max_Occupancy_prob) %>% 
  hist()
#Vast majority of ASVs were detected only in 1 technical replicate.

#Summarize
mpa_occupancy_results_all %>% 
  ungroup() %>% 
  mutate(., to_keep = case_when(tot_rep < 2 ~ "Toss",
                                max_Occupancy_prob > 0.90 ~ "Keep",
                                TRUE ~ "Borderline")) %>% 
  count(to_keep) %>% 
  mutate(., total=sum(n)) %>% 
  mutate(per=paste0(round(100*n/total,2),'%')) %>% 
  dplyr::select(-total)

#Will include all ASVs 75% occupancy for the time being.


```

So we will throw away most of the Hashes, but will keep most of the reads - we are getting into something here

```{r actual filtering}
metadata %>% 
  mutate(., sample=Samples) -> metadata

ASV.nested %>% 
  mutate (Step2.tibble.edited = purrr::map(Step2.tibble, 
                              function(.x){
                                .x %>%
                                  left_join(metadata) %>%
                                  mutate(seq_number=Hash) %>% 
                                  left_join(Hash.key) %>% 
                                  unite(sum.taxonomy,Site, col="HashSite") %>% 
                                  dplyr::select(sample, Hash, nReads, HashSite)
                                }) ) -> ASV.nested

ASV.nested$Step2.tibble.edited[[1]] %>% 
  left_join(mpa_occupancy_results_all) %>% 
  mutate(., seq_number = Hash) %>% 
  left_join(Hash.key) %>% 
  filter(., sum.taxonomy == "Eukaryota;Chordata;Actinopteri;Perciformes;Sebastidae;Sebastes;")

ASV.nested %>% 
mutate(Step3.tibble = purrr::map (Step2.tibble.edited, ~ filter(.,HashSite %in% mpa_occupancy_results_75$HashSite) %>% ungroup))-> ASV.nested

ASV.nested$Step3.tibble[[1]] %>% dplyr::select(HashSite) %>%  unique()

ASV.nested %>% 
  transmute(Miseq_run, Summary.1 = purrr::map(Step3.tibble, ~ how.many(ASVtable = .,round ="4.Occupancy"))) %>% 
  left_join(ASV.summary) %>%
  bind_cols(ASV.summary) %>%
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary

ASV.summary$Summary

```

```{r}

saveRDS(ASV.nested,file="ASV.nested_post_occ.RDS")
saveRDS(ASV.summary,file="ASV.summary_post_occ.RDS")

ASV.nested <- readRDS(file ="ASV.nested_post_occ.RDS")
ASV.summary <- readRDS(file ="ASV.summary_post_occ.RDS")

```


```{r Summary MPA only post occ}
ASV.nested %>% 
  mutate (Step2.tibble.edited = purrr::map(Step2.tibble, 
                              function(.x){
                                .x %>%
                                  left_join(metadata) %>%
                                  filter(!sample %in% Taylor_samples) %>%                                                        mutate(seq_number=Hash) %>% left_join(Hash.key) %>%
                                  unite(sum.taxonomy,Site, col="HashSite") %>% 
                                  dplyr::select(sample, Hash, nReads, HashSite)
                                }) ) -> ASV.nested.mpa

ASV.nested.mpa$Step2.tibble.edited[[1]]

ASV.nested.mpa %>% 
mutate(Step3.tibble = purrr::map (Step2.tibble.edited, ~ filter(.,HashSite %in% mpa_occupancy_results_75$HashSite) %>% ungroup))-> ASV.nested.mpa

ASV.nested.mpa$Step3.tibble[[1]]

ASV.nested.mpa %>% 
  transmute(Miseq_run, Summary.1 = purrr::map(Step3.tibble, ~ how.many(ASVtable = .,round ="4.Occupancy"))) %>% 
  left_join(ASV.summary.mpa_1) %>%
  bind_cols(ASV.summary.mpa_1) %>%
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary.mpa_1

ASV.summary.mpa_1$Summary

```

## Cleaning Process 5: **Dissimilarity between PCR (biological) replicates**

So, a second way of cleaning the dataset is to remove samples for which the dissimilarity between PCR replicates exceeds the normal distribution of dissimilarities.
Sometimes the preparation of a PCR replicate goes wrong for a number of reasons - that leads to a particular PCR replicate to be substantially different to the other 2. In that case, we will remove the PCR replicate that has higher dissimilarity with the other two.

The process starts by adding the biological information to the ASV table, then diving the dataset by their biological replicate. This will also remove any sample that is not included in the metadata, eg coming from a different project.

```{r dissimilarity between PCR replicates}
ASV.nested.mpa$Step3.tibble %>%
  as.data.frame() %>% 
  group_by(sample, Hash) %>% 
  mutate(., tot_reads = sum(nReads)) %>% 
  filter(., tot_reads >1) %>% 
  left_join(metadata)-> cleaned.tibble.mpa.post.occ

ASV.nested.mpa$Step2.tibble.edited %>%
  as.data.frame() %>% 
  ungroup() %>% 
  left_join(metadata) -> cleaned.tibble.mpa.pre.occ
```


```{r quick check}
# do all samples have a name
cleaned.tibble.mpa.post.occ %>% 
  filter (sample == "")

cleaned.tibble.mpa.pre.occ %>% 
  filter (sample == "")

# do all of them have an original sample
cleaned.tibble.mpa.post.occ %>% 
  filter(Site == "")

cleaned.tibble.mpa.pre.occ %>% 
  filter(Site == "")

# do all of them have a Hash
cleaned.tibble.mpa.post.occ %>% 
  filter(is.na(Hash))

cleaned.tibble.mpa.pre.occ %>% 
  filter(is.na(Hash))

# How many samples, how many Hashes
cleaned.tibble.mpa.post.occ %>% 
  dplyr::summarise(n_distinct(sample), #27
            n_distinct(Hash))  #2086

cleaned.tibble.mpa.pre.occ %>% 
  dplyr::summarise(n_distinct(sample), # 27
            n_distinct(Hash)) #16428

# Let's check the levels of replication

cleaned.tibble.mpa.post.occ %>% 
  group_by(Site,Location) %>% 
  dplyr::summarise(nrep = n_distinct(sample)) %>%
  filter (nrep == 3) #all
  #filter (nrep == 2) # 0
  #filter (nrep == 1) # 0

cleaned.tibble.mpa.pre.occ %>% 
  group_by(Site,Location) %>% 
  dplyr::summarise(nrep = n_distinct(sample)) %>%
  filter (nrep == 3) #all
  #filter (nrep == 2) # 0
  #filter (nrep == 1) # 0

```

```{r remove single replicates}

discard.1.single <- cleaned.tibble.mpa.post.occ %>% 
  group_by(Site,Location) %>% 
  mutate(nrep = n_distinct(sample)) %>% 
  #filter (nrep == 2) # 25
  filter (nrep == 1) %>% 
  distinct(sample) %>% pull(sample)

cleaned.tibble.mpa.post.occ %>% 
  filter(!sample %in% discard.1.single) -> cleaned.tibble.mpa.post.occ

discard.1 <- cleaned.tibble.mpa.pre.occ %>% 
  group_by(Site,Location) %>% 
  mutate(nrep = n_distinct(sample)) %>% 
  #filter (nrep == 2) # 25
  filter (nrep == 1) %>% 
  distinct(sample) %>% pull(sample)

cleaned.tibble.mpa.pre.occ %>% 
  filter(!sample %in% discard.1) -> cleaned.tibble.mpa.pre.occ

```

Anyway, let's have a visual representation of the dissimilarities between PCR replicates, biological replicates and everything else.

```{r lets do the PCR replication}
cleaned.tibble.mpa.post.occ %>%
  dplyr::group_by (sample) %>%
  mutate (Tot = sum(nReads),
          Row.sums = nReads / Tot) %>% 
  dplyr::group_by (Hash) %>%
  mutate (Colmax = max (Row.sums),
          Normalized.reads = Row.sums / Colmax) -> cleaned.tibble.mpa.post.occ #transforms raw number of reads to eDNA index

cleaned.tibble.mpa.pre.occ %>%
  dplyr::group_by (sample) %>%
  mutate (Tot = sum(nReads),
          Row.sums = nReads / Tot) %>% 
  dplyr::group_by (Hash) %>%
  mutate (Colmax = max (Row.sums),
          Normalized.reads = Row.sums / Colmax) -> cleaned.tibble.mpa.pre.occ #transforms raw number of reads to eDNA index

tibble_to_matrix <- function (tb) {
  
  tb %>% 
    group_by(sample, Hash) %>% 
    dplyr::summarise(nReads = sum(Normalized.reads)) %>% 
    spread ( key = "Hash", value = "nReads", fill = 0) -> matrix_1
    samples <- pull (matrix_1, sample)
    matrix_1 %>% 
      ungroup() %>% 
    dplyr::select ( - sample) -> matrix_1
    data.matrix(matrix_1) -> matrix_1
    dimnames(matrix_1)[[1]] <- samples
    vegdist(matrix_1) -> matrix_1
}

tibble_to_matrix (cleaned.tibble.mpa.post.occ) -> all.distances.full.mpa.post.occ
tibble_to_matrix (cleaned.tibble.mpa.pre.occ) -> all.distances.full.mpa.pre.occ

#names(all.distances.full)

summary(is.na(names(all.distances.full.mpa.post.occ)))
summary(is.na(names(all.distances.full.mpa.pre.occ)))

```

Let's make the pairwaise distances a long table
```{r}

as.tibble(subset(melt(as.matrix(all.distances.full.mpa.post.occ)))) -> all.distances.melted.mpa.post.occ

as.tibble(subset(melt(as.matrix(all.distances.full.mpa.pre.occ)))) -> all.distances.melted.mpa.pre.occ

summary(is.na(all.distances.melted.mpa.post.occ$value))
summary(is.na(all.distances.melted.mpa.pre.occ$value))

```

```{r}
# Now, create a three variables for all distances, they could be PCR replicates, BIOL replicates, or from the same site

all.distances.melted.mpa.post.occ %>%
  separate(Var1, into = c("Site1","Location1", "rep1") , sep = "\\.", remove = F) %>% 
  separate(Var2, into = c("Site2","Location2", "rep2") , sep = "\\.", remove = F) %>% 
  unite( Site1, Location1, col= "station1", remove=F) %>% 
  unite( Site2, Location2, col= "station2", remove=F) %>% 
  mutate(Distance.type = case_when(station1 == station2 ~ "Biological.replicates",
                                      Site1 == Site2 ~ "Same Site",
                                      TRUE ~ "Different Site"
                                     )) %>%
  dplyr::select(Sample1 = Var1, Sample2 = Var2 , value , Distance.type) %>%
  filter (Sample1 != Sample2) -> all.distances.to.plot.mpa.post.occ


# Checking all went well
sapply(all.distances.to.plot.mpa.post.occ, function(x) summary(is.na(x)))

all.distances.to.plot.mpa.post.occ$Distance.type <- all.distances.to.plot.mpa.post.occ$Distance.type  %>% fct_relevel( "PCR.replicates", "Biol.replicates", "Same Site")

  ggplot (all.distances.to.plot.mpa.post.occ , aes (fill = Distance.type, x = value)) +
  geom_histogram (position = "dodge", stat = 'density', alpha = 0.9) +
 # facet_wrap( ~ Distance.type) +
 labs (x = "Pairwise Dissimilarity", y = "Density" ,
        fill = "Groups", title = "eDNA Pairwise Dissimilarity") +theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank())
  
```

```{r}
all.distances.melted.mpa.pre.occ %>%
  separate(Var1, into = c("Site1","Location1", "rep1") , sep = "\\.", remove = F) %>% 
  separate(Var2, into = c("Site2","Location2", "rep2") , sep = "\\.", remove = F) %>% 
  unite( Site1, Location1, col= "station1", remove=F) %>% 
  unite( Site2, Location2, col= "station2", remove=F) %>% 
  mutate(Distance.type = case_when(station1 == station2 ~ "Biological.replicates",
                                      Site1 == Site2 ~ "Same Site",
                                      TRUE ~ "Different Site"
                                     )) %>%
  dplyr::select(Sample1 = Var1, Sample2 = Var2 , value , Distance.type) %>%
  filter (Sample1 != Sample2) -> all.distances.to.plot.pre.occ

# Checking all went well

sapply(all.distances.to.plot.pre.occ, function(x) summary(is.na(x)))

all.distances.to.plot.pre.occ$Distance.type <- all.distances.to.plot.pre.occ$Distance.type  %>% fct_relevel( "PCR.replicates", "Biol.replicates", "Same Site")

  ggplot (all.distances.to.plot.pre.occ , aes (fill = Distance.type, x = value)) +
  geom_histogram (position = "dodge", stat = 'density', alpha = 0.9) + 
 # facet_wrap( ~ Distance.type) +
 labs (x = "Pairwise Dissimilarity", y = "Density" ,
        fill = "Groups", title = "eDNA Pairwise Dissimilarity") +theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank())
  
  
  
```

#Code for merging ASV tables
```{r}
#Hashes Unique Species
Hash.key %>% 
  distinct(.,sum.taxonomy) -> hashes_unique

hashes_unique$number <- row.names(hashes_unique)
hashes_unique$number <- paste0("taxon_",hashes_unique$number)
row.names(hashes_unique)<-hashes_unique$number

Hash.key %>% 
  left_join(hashes_unique, by="sum.taxonomy") -> Hash.key.updated

head(Hash.key.updated)

#Create Data List for merging taxon files
Hash.key.updated %>% 
  mutate(Seq_number=seq_number) -> Hash.key.updated

head(Hash.key.updated)
```
#Function Merge
```{r}


#Function for standardizing and merging eDNA taxon data.frames

StdIndex <- function (x) {
  
  #Input: a LIST of objects, each of which is an eDNA taxon-read-count dataframe dataset. 
  #Output: a list of two data.frames: single data.frame of taxon indices (each representing an ensemble index, an average of the  input data), and a second data.frame with standard errors for those ensemble estimates
  
  #Assumes taxa are in rows (with taxon name as row.name) and samples/sites are in columns. 
  #Also assumes column names are consistent across datasets. 
  
  #dependencies: vegan
  
  SE<-function(x) sd(x, na.rm=T)/sqrt(sum(!is.na(x))) #calculate standard error of the mean
  Col2RN<-function(df, x){row.names(df)<-df[,x]; df<-df[-x]; return(df)} #convert column to row.names
  
  # step 1: standardize taxon tables using wisonsin double-standardization
  stdList<-lapply(x, vegan::wisconsin)
  taxvec<-unlist(lapply(stdList, row.names))
  
  #step 2: aggregate by taxon name and calculate FUN (by default, mean)
  taxonMeans<-aggregate(do.call(rbind, stdList), 
                        by=list(taxvec), 
                        FUN = mean, na.rm=T)
  taxonMeans<-Col2RN(taxonMeans, 1)
  
  taxonSE<-aggregate(do.call(rbind, stdList),
                     by=list(taxvec),
                     FUN = SE)
  taxonSE<-Col2RN(taxonSE, 1)
  
  return(list(IndexValues = taxonMeans, IndexSE =taxonSE))
}
```
#Post Occupancy
```{r}
ASV.nested.mpa$Step3.tibble[[1]] %>% 
  mutate(Seq_number=Hash) %>% 
   filter(., str_detect(Hash,"merged")) %>% 
    filter(., !str_detect(Hash,"unmerged")) %>% 
  left_join(Hash.key.updated, by="Seq_number") %>% 
  dplyr::group_by(number,sample) %>%
  dplyr::summarise(nReads=sum(nReads)) %>% 
  spread(., sample, nReads) %>% #convert to wide data format
  replace(is.na(.), 0) -> mpa_step3

metadata %>% 
  filter(., !(Samples %in% controls)) %>% 
  filter(., !(Samples %in% Taylor_samples)) %>% 
  filter(.,!(Samples %in% colnames(mpa_step3))) %>% 
  pull(Samples) -> columns2add

mpa_step3 <- as.data.frame(mpa_step3)

mpa_step3 %>% 
     tibble::add_column(!!!set_names(as.list(rep(NA, length(columns2add))),nm=columns2add)) %>% 
     replace(is.na(.), 0) -> mpa_step3

row.names(mpa_step3) <- mpa_step3$number
mpa_step3 %>% ungroup() %>% dplyr::select(-number) -> mpa_step3

dim(mpa_step3)

datalist_mpa <- list(mpa_step3)
```

```{r}
results_mpa<-StdIndex(datalist_mpa)

dim(results_mpa$IndexValues)

Hash.key.updated.2 <- Hash.key.updated[!duplicated(Hash.key.updated$number), ]

results_mpa[[1]]$number <- rownames(results_mpa[[1]])

results_mpa[[1]] %>% 
  left_join(Hash.key.updated.2, by="number") %>% 
  dplyr::select(-number,-seq_number,-Seq_number) -> results_mpa[[1]]

results_mpa[[2]]$number <- rownames(results_mpa[[2]])

results_mpa[[2]] %>% 
  left_join(Hash.key.updated.2, by="number") %>% 
  dplyr::select(-number,-seq_number,-Seq_number) -> results_mpa[[2]]

saveRDS(results_mpa,file="mpa_post_occ_75_sum.taxonomy_e_index.RDS")
write_csv(results_mpa[[1]] ,"mpa_post_occ_75_sum.taxonomy_e_index.csv")

```
#Pre Occupancy
```{r}

ASV.nested.mpa$Step2.tibble.edited[[1]] %>% 
    filter(., !sample %in% Taylor_samples) %>%
   filter(., str_detect(Hash,"merged")) %>% 
    filter(., !str_detect(Hash,"unmerged")) %>% 
  mutate(Seq_number=Hash) %>% 
  left_join(Hash.key.updated, by="Seq_number") %>% 
  dplyr::group_by(number,sample) %>%
  dplyr::summarise(nReads=sum(nReads)) %>% 
  filter(., nReads >1) %>% 
  spread(., sample, nReads) %>% #convert to wide data format
  replace(is.na(.), 0) -> mpa_step2

metadata %>% 
  filter(., !(Samples %in% controls)) %>% 
  filter(., !(Samples %in% Taylor_samples)) %>% 
  filter(.,!(Samples %in% colnames(mpa_step2))) %>% 
  pull(Samples) -> columns2add

mpa_step2 <- as.data.frame(mpa_step2)

mpa_step2 %>% 
     tibble::add_column(!!!set_names(as.list(rep(NA, length(columns2add))),nm=columns2add)) %>% 
     replace(is.na(.), 0) -> mpa_step2

row.names(mpa_step2) <- mpa_step2$number
mpa_step2 %>% ungroup() %>% dplyr::select(-number) -> mpa_step2

dim(mpa_step2)

datalist_mpa_pre_occ <- list(mpa_step2)
```

```{r}

results_mpa_lenient<-StdIndex(datalist_mpa_pre_occ)

dim(results_mpa_lenient$IndexValues)

Hash.key.updated.2 <- Hash.key.updated[!duplicated(Hash.key.updated$number), ]

results_mpa_lenient[[1]]$number <- rownames(results_mpa_lenient[[1]])

results_mpa_lenient[[1]] %>% 
  left_join(Hash.key.updated.2, by="number") %>% 
  dplyr::select(-number,-seq_number,-Seq_number) -> results_mpa_lenient[[1]]

results_mpa_lenient[[2]]$number <- rownames(results_mpa_lenient[[2]])

results_mpa_lenient[[2]] %>% 
  left_join(Hash.key.updated.2, by="number") %>% 
  dplyr::select(-number,-seq_number,-Seq_number) -> results_mpa_lenient[[2]]

saveRDS(results_mpa_lenient,file="mpa_pre_occ_sum.taxonomy_e_index.RDS")
write_csv(results_mpa_lenient[[1]] ,"mpa_pre_occ_sum.taxonomy_e_index.csv")
```


#Merge FishCARD Updated
```{r}

fish_hash_path <- "/Users/zackgold/Documents/UCLA_phd/Projects/California/General_Channel_Islands/MPA_test_data/input/2020320_analysis/analysis_20200330/hashes_fishcard_040920.txt"

#Read in Tables
fish_hash <- read.table(fish_hash_path, header = 1, sep = "\t", stringsAsFactors = F)

#Hashes Unique Species
fish_hash %>% 
  distinct(.,sum.taxonomy) -> hashes_unique_fish

hashes_unique_fish$number <- row.names(hashes_unique_fish)
hashes_unique_fish$number <- paste0("taxon_",hashes_unique_fish$number)
row.names(hashes_unique_fish)<-hashes_unique_fish$number

fish_hash %>% 
  left_join(hashes_unique_fish, by="sum.taxonomy") -> Hash.key.updated.fish

head(Hash.key.updated.fish)

#Create Data List for merging taxon files
Hash.key.updated.fish %>% 
  mutate(Seq_number=seq_number) -> Hash.key.updated.fish

head(Hash.key.updated.fish)

saveRDS(Hash.key.updated.fish,file="Hash.key.updated.fish.RDS")

```

#Fish Post Occupancy
```{r}
ASV.nested.mpa$Step3.tibble[[1]] %>% 
  mutate(Seq_number=Hash) %>% 
   filter(., str_detect(Hash,"merged")) %>% 
    filter(., !str_detect(Hash,"unmerged")) %>% 
  left_join(Hash.key.updated.fish, by="Seq_number") %>% 
  dplyr::group_by(number,sample) %>%
  dplyr::summarise(nReads=sum(nReads)) %>% 
  spread(., sample, nReads) %>% #convert to wide data format
  replace(is.na(.), 0) -> mpa_step3

metadata %>% 
  filter(., !(Samples %in% controls)) %>% 
  filter(., !(Samples %in% Taylor_samples)) %>% 
  filter(.,!(Samples %in% colnames(mpa_step3))) %>% 
  pull(Samples) -> columns2add

mpa_step3 <- as.data.frame(mpa_step3)

mpa_step3 %>% 
     tibble::add_column(!!!set_names(as.list(rep(NA, length(columns2add))),nm=columns2add)) %>% 
     replace(is.na(.), 0) -> mpa_step3

row.names(mpa_step3) <- mpa_step3$number
mpa_step3 %>% ungroup() %>% dplyr::select(-number) -> mpa_step3

dim(mpa_step3)

datalist_mpa <- list(mpa_step3)
```

```{r}
results_mpa<-StdIndex(datalist_mpa)

dim(results_mpa$IndexValues)

Hash.key.updated.2 <- Hash.key.updated.fish[!duplicated(Hash.key.updated.fish$number), ]

results_mpa[[1]]$number <- rownames(results_mpa[[1]])

results_mpa[[1]] %>% 
  left_join(Hash.key.updated.2, by="number") %>% 
  dplyr::select(-number,-seq_number,-Seq_number) -> results_mpa[[1]]

results_mpa[[2]]$number <- rownames(results_mpa[[2]])

results_mpa[[2]] %>% 
  left_join(Hash.key.updated.2, by="number") %>% 
  dplyr::select(-number,-seq_number,-Seq_number) -> results_mpa[[2]]

saveRDS(results_mpa,file="mpa_post_occ_75_sum.taxonomy_e_index_fishcard.RDS")
write_csv(results_mpa[[1]] ,"mpa_post_occ_75_sum.taxonomy_e_index_fishcard.csv")

```
#Pre Occupancy
```{r}

ASV.nested.mpa$Step2.tibble.edited[[1]] %>% 
    filter(., !sample %in% Taylor_samples) %>%
   filter(., str_detect(Hash,"merged")) %>% 
    filter(., !str_detect(Hash,"unmerged")) %>% 
  mutate(Seq_number=Hash) %>% 
  left_join(Hash.key.updated.fish, by="Seq_number") %>% 
  dplyr::group_by(number,sample) %>%
  dplyr::summarise(nReads=sum(nReads)) %>% 
  filter(., nReads >1) %>% 
  spread(., sample, nReads) %>% #convert to wide data format
  replace(is.na(.), 0) -> mpa_step2

metadata %>% 
  filter(., !(Samples %in% controls)) %>% 
  filter(., !(Samples %in% Taylor_samples)) %>% 
  filter(.,!(Samples %in% colnames(mpa_step2))) %>% 
  pull(Samples) -> columns2add

mpa_step2 <- as.data.frame(mpa_step2)

mpa_step2 %>% 
     tibble::add_column(!!!set_names(as.list(rep(NA, length(columns2add))),nm=columns2add)) %>% 
     replace(is.na(.), 0) -> mpa_step2

row.names(mpa_step2) <- mpa_step2$number
mpa_step2 %>% ungroup() %>% dplyr::select(-number) -> mpa_step2

dim(mpa_step2)

datalist_mpa_pre_occ <- list(mpa_step2)
```

```{r}

results_mpa_lenient<-StdIndex(datalist_mpa_pre_occ)

dim(results_mpa_lenient$IndexValues)

Hash.key.updated.2 <- Hash.key.updated.fish[!duplicated(Hash.key.updated.fish$number), ]

results_mpa_lenient[[1]]$number <- rownames(results_mpa_lenient[[1]])

results_mpa_lenient[[1]] %>% 
  left_join(Hash.key.updated.2, by="number") %>% 
  dplyr::select(-number,-seq_number,-Seq_number) -> results_mpa_lenient[[1]]

results_mpa_lenient[[2]]$number <- rownames(results_mpa_lenient[[2]])

results_mpa_lenient[[2]] %>% 
  left_join(Hash.key.updated.2, by="number") %>% 
  dplyr::select(-number,-seq_number,-Seq_number) -> results_mpa_lenient[[2]]

saveRDS(results_mpa_lenient,file="mpa_pre_occ_sum.taxonomy_e_index_fishcard.RDS")
write_csv(results_mpa_lenient[[1]] ,"mpa_pre_occ_sum.taxonomy_e_index_fishcard.csv")

results_mpa_lenient[[1]] %>%  View()
```

